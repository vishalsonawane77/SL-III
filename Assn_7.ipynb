{"cells":[{"cell_type":"markdown","source":["1. Extract Sample document and apply following document preprocessing methods: tokenization, POS Tagging, stop words removal, stemming and lemmatization\n","2. Create repesentation of documents by calculating Term Frequency and InverseDocumentFrequency"],"metadata":{"id":"f6dILJPPm1XA"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"t0v-YgzjmgX8"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":1,"metadata":{"id":"Jw1p0dnvBkmy","executionInfo":{"status":"ok","timestamp":1712981452347,"user_tz":-330,"elapsed":2721,"user":{"displayName":"Riya Jain","userId":"05653870576225209853"}}},"outputs":[],"source":["import nltk\n","from nltk import word_tokenize, sent_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import SnowballStemmer\n","from  nltk.stem import WordNetLemmatizer\n","from sklearn.feature_extraction.text import TfidfVectorizer"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L5Rxk5fXBqe3","outputId":"8bb24816-eaf0-4b20-a068-103d4ecb3f70","executionInfo":{"status":"ok","timestamp":1712981681246,"user_tz":-330,"elapsed":653,"user":{"displayName":"Riya Jain","userId":"05653870576225209853"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"stream","name":"stdout","text":["['Hello', 'I', 'am', 'Gayatri', 'Deshmukh', '.', 'I', 'am', 'from', 'Nanded', 'District', '.', 'I', 'will', 'be', 'an', 'Engineer', 'in', 'few', 'months', '.']\n","['Hello I am Gayatri Deshmukh.', 'I am from Nanded District.', 'I will be an Engineer in few months.']\n"]}],"source":["sentence = \"Hello I am Gayatri Deshmukh. I am from Nanded District. I will be an Engineer in few months.\"\n","\n","#Tokenization\n","nltk.download('punkt')\n","tokenized_words = word_tokenize(sentence)\n","tokenized_sentences = sent_tokenize(sentence)\n","\n","print(tokenized_words)\n","print(tokenized_sentences)"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aFOa2ViiC9iU","outputId":"0d06dfe6-6566-43a0-9964-a8386690c10a","executionInfo":{"status":"ok","timestamp":1712981683338,"user_tz":-330,"elapsed":350,"user":{"displayName":"Riya Jain","userId":"05653870576225209853"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Unclean version  ['Hello', 'I', 'am', 'Gayatri', 'Deshmukh', '.', 'I', 'am', 'from', 'Nanded', 'District', '.', 'I', 'will', 'be', 'an', 'Engineer', 'in', 'few', 'months', '.']\n","Clean Version ['Hello', 'I', 'Gayatri', 'Deshmukh', '.', 'I', 'Nanded', 'District', '.', 'I', 'Engineer', 'months', '.']\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}],"source":["#Stop words removal\n","nltk.download('stopwords')\n","stop_words = stopwords.words('english')\n","\n","cleaned_token = []\n","for i in tokenized_words:\n","  if i not in stop_words:\n","    cleaned_token.append(i)\n","\n","print(\"Unclean version \", tokenized_words)\n","print(\"Clean Version\", cleaned_token)"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"_UFln4zoOQf8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712981687423,"user_tz":-330,"elapsed":365,"user":{"displayName":"Riya Jain","userId":"05653870576225209853"}},"outputId":"2b2be4d1-4726-4c28-adfc-a5942014909a"},"outputs":[{"output_type":"stream","name":"stdout","text":["['hello', 'i', 'am', 'gayatri', 'deshmukh', '.', 'i', 'am', 'from', 'nand', 'district', '.', 'i', 'will', 'be', 'an', 'engin', 'in', 'few', 'month', '.']\n"]}],"source":["#Stemming\n","snowball_stemmer = SnowballStemmer('english')\n","\n","stemmed_words = []\n","for i in tokenized_words:\n","    stemmed = snowball_stemmer.stem(i)\n","    stemmed_words.append(stemmed)\n","\n","print(stemmed_words)"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XKbYLwAbq4zl","outputId":"69ed31e6-d2ae-4ce4-ad0c-7d0a8f4bb94a","executionInfo":{"status":"ok","timestamp":1712981708744,"user_tz":-330,"elapsed":2445,"user":{"displayName":"Riya Jain","userId":"05653870576225209853"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]},{"output_type":"stream","name":"stdout","text":["['Hello', 'I', 'am', 'Gayatri', 'Deshmukh', '.', 'I', 'am', 'from', 'Nanded', 'District', '.', 'I', 'will', 'be', 'an', 'Engineer', 'in', 'few', 'month', '.']\n"]}],"source":["#Lemmatization\n","nltk.download('wordnet')\n","wordnet_lemmatizer = WordNetLemmatizer()\n","\n","lemmatized_words = []\n","for i in tokenized_words:\n","    lemmatized = wordnet_lemmatizer.lemmatize(i)\n","    lemmatized_words.append(lemmatized)\n","\n","print(lemmatized_words)"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6ei5-kNvrFrZ","outputId":"8819f2c1-524d-42a9-e26a-60a192034a2a","executionInfo":{"status":"ok","timestamp":1712981714301,"user_tz":-330,"elapsed":994,"user":{"displayName":"Riya Jain","userId":"05653870576225209853"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"]},{"output_type":"stream","name":"stdout","text":["[('Hello', 'NNP'), ('I', 'PRP'), ('am', 'VBP'), ('Gayatri', 'NNP'), ('Deshmukh', 'NNP'), ('.', '.'), ('I', 'PRP'), ('am', 'VBP'), ('from', 'IN'), ('Nanded', 'NNP'), ('District', 'NNP'), ('.', '.'), ('I', 'PRP'), ('will', 'MD'), ('be', 'VB'), ('an', 'DT'), ('Engineer', 'NNP'), ('in', 'IN'), ('few', 'JJ'), ('months', 'NNS'), ('.', '.')]\n"]}],"source":["#Pos Tagging\n","# dt - determinnant\n","# NN - noun\n","# In - prep / conjunc\n","nltk.download('averaged_perceptron_tagger')\n","pos_tag = nltk.pos_tag(tokenized_words)\n","\n","print(pos_tag)"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"1_ITg5Ud09Hu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712981854419,"user_tz":-330,"elapsed":357,"user":{"displayName":"Riya Jain","userId":"05653870576225209853"}},"outputId":"5a05833d-c649-4825-b65f-fd75b0cb48a5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Word Indexing:  {'good': 4, 'morning': 8, 'do': 1, 'daily': 0, 'exercise': 2, 'in': 6, 'the': 9, 'is': 7, 'for': 3, 'health': 5}\n","tf-idf in matrix form: \n"," [[0.         0.         0.         0.         0.70710678 0.\n","  0.         0.         0.70710678 0.        ]\n"," [0.44036207 0.44036207 0.3349067  0.         0.         0.\n","  0.44036207 0.         0.3349067  0.44036207]\n"," [0.         0.         0.37302199 0.49047908 0.37302199 0.49047908\n","  0.         0.49047908 0.         0.        ]]\n"]}],"source":["d0 = \"Good Morning\"\n","d1 = \"Do daily exercise in the morning \"\n","d2 = \"exercise is good for health\"\n","series = [d0, d1, d2]\n","tfidf = TfidfVectorizer()\n","result = tfidf.fit_transform(series)\n","\n","print(\"Word Indexing: \", tfidf.vocabulary_)\n","print(\"tf-idf in matrix form: \\n\", result.toarray())"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"}},"nbformat":4,"nbformat_minor":0}